{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7937b131-b4e4-4b79-87b9-f71a1bbcc52d",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4d62f-a823-4543-b7ab-45839d7d7c27",
   "metadata": {},
   "source": [
    "One way to apply deep learning to object detection is to use region-based CNNs also known as R-CNNs. The first R-CNN comprised of three stages. The first stage scans the image and identifies up to 2.000 bounding boxes representing regions of interest or regions that might contain objects. The second stage is a deep CNN that extracts features from regions of interest and the third is a support vector machine that classifies the features. The output is a collection of bounding boxes with class labels and confidence scores. An algorithm called non-maximum supression (NMS) filters the output and selects the best bounding box for each object.\n",
    "\n",
    "A detector emits everal bounding boxes for each objet, if a photo contains on instance of agiven class NMS selects the bounding box with the highest confidence score. I the photo contains two objects of the same class NMS divides the bounding boxes ito groups and selects the box with the hightest confidence score in each group. It groups the boxes based on the amout of overlap between them. Overlap is computed by dividing the area of intersection between two boxes by the area formed by the union of the boxes. If the resulting intersection-over-union (IoU) score is greater than the predetermined threshold typicaly 0.5 NMS assigns the boxes to the same group, otherwise it assigns them to separate groups. If two obbjects of the same class have little or no overlap NMS easily separates the two but if two instances of the same class overlap significanly the IoU threshold might have to be adjuxted to achieve separation.\n",
    "\n",
    "The first stage of most R-CNN implementations uses an algorithm called selective search to identify regions of interest by keying on similarities in color, texture, shape, and size. Even with selective search narrowing the list of candidate regions input to stage 2 an R-CNN can't do object detection in real time because the CNN individually processes the 2.000 or so regions of interests identified in stage 1. \n",
    "\n",
    "A paper titled \"Fast R-CNN\" adresses this by proposing a modified architecture in which the entire image passes through the CNN one time. Selective search or another similar algorithm identifies regions of interest and those regions are projected onto the feature map generated by the CNN. A region of interest (ROI) pooling layer then uses a form of max pooling to reduce the features in each region of interest to a fixed-length vector independent of the region's size and shape. Classification of the feature vector is performed by fully connected layers rather than SVMs and the output is split to include both a softmax classifier and a bounding-box regressor.\n",
    "\n",
    "A paper titled \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" further boosed performance by replacing selective search with a region proposal network or RPN which is a shallow cNN that shares layers with the main CNN. To generate region proposals, it slides a window over the feature map generated by the last shared layer, at each stop in the window's travel the RPN evaluates n candidate regions called anchors or anchor boxes and computes an objectness score for each based on IoU scores with ground-truth boxes. Objectness scores and anchor boxes are input to fully connected layers for classification and regression. The output from these layers ultimately determines the regions of interest projected onto the feature map generated by the main CNN and forwarded to the ROI pooling layer.\n",
    "\n",
    "Mask R-CNNs introduced in the paper \"Mask R-CNN\" extend Faster R-CNN by adding instance segmentation, which identifies the shapes of objects detected in an image using segmentation masks. Impact on performance is minimal with the benefit of providing more detail about the objects detected. They also replace ROI pooling with ROI alignment which discards less information when generating feature vectors whose boundaries don't perfectlly align with the boundaries of the regions they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df9dbf-9579-4daf-83cd-939f5feb634b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
