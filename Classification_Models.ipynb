{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0bf519-d2ed-407f-be01-49f203c592ce",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee834b-e513-4bf5-856e-f2aa95849710",
   "metadata": {},
   "source": [
    "There are two main categories of classification models:\n",
    "1. Binary classification models - there are only two possible outcomes\n",
    "2. Multiclass classification - there are more than two possible outcomes\n",
    "\n",
    "There are also other less common categories like multilable classification models which can classify a single input as belonging to several classes. There are also some models which can predict that an input belongs to none of the possible classes.\n",
    "\n",
    "Most of the things that are important in regression also apply to classification models, with the main difference being the way that accuracy is measured. In classification there isn't a R-squared metric but in its place are other measured like precision, recall, specificity, sensitivity and F1 score. One of the most important things in classification is to know which accuracy metric to use based on the model's intended application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf71c2-b1a1-4af5-9e50-c6e67f4aa0fd",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb22a7-fe77-4ce9-896d-e785fc1f04a8",
   "metadata": {},
   "source": [
    "Logistic Regression is an algorithm that is used only for classification. It analyzes a distribution of data and fits an equation to it that defines the porbability that a given sample belogns to each of the possible classes and it predicts that it belongs to the class which has a higher probability. Despite the name logistic regression is used for classificatin, its purpose is not to create regression models but to quantify probabilities for the purpose of classifying input samples.\n",
    "\n",
    "The algorithm draws a curve that given an x shows the probability that a point with that x belongs to class 1. It charts a function known as the logistic function (also known as the logit function). The function for logistic regression is defined as:\n",
    "\n",
    "$ y = \\frac{1}{1+e^{-(mx + b)}}$\n",
    "\n",
    "Where x is the input value and m and b are parameters learned during training. The reason it is called logistic regression is that the exponent of e happens to be the equation for linear regression.\n",
    "\n",
    "To find the optimum values Scikit defaults to a numberical optimization algorithm known as Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e156b86-7cf8-46d8-8042-f39de0a6b84d",
   "metadata": {},
   "source": [
    "## Accuracy Measures for Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20adffa-788b-436d-b810-e5c6a691e0ba",
   "metadata": {},
   "source": [
    "For a classifier the score function returns the sum of the true positives and the true negatives divided by the total number of samples. This is sometimes referred to as the model's accuracy score. Often data scientists measure a classification model's precision and recall.\n",
    "\n",
    "1. Precision: Computed by dividing the number of true positives by the sum of the true positives and false positives\n",
    "2. Recall: Computed by dividing the number of true positives by the sum of true positives and false negatives\n",
    "\n",
    "Precision imposes a penalty for false positives while recall penalizes false negatives. Precision quantifies how confident you can be that a positive prediction is accurate, while recall quantifies the model's ability to accurately identify positive samples. The two can be combined into one score called the F1 score.\n",
    "\n",
    "The decision on which one to use comes to deciding on the cost of false positives or the cost of false negatives. If the cost of false positives is high then percision should be used. If the cost of false negatives is high then recall should be used.\n",
    "\n",
    "An additional metric that applies only to binary classification is the receiver operating characteristic (ROC) curve, which plots the true-positive rate (TPR) agains the false-positive rate (FPR) at various probability thresholds. Data scientists often use the area under the curve (AUC or ROC AUC) as an overall measure of accuracy.\n",
    "\n",
    "Another way to assess the accuracy of a classification model is to plot a confusion matrix. It shows for each class how the model performed during testing.\n",
    "\n",
    "Other metrics include sensitivity and specificity. Sensitivity is identical to recall. Specificity is recall for negative class rather than positive class and it is calculted by dividing the number of true negatives by the sum of true negatives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1bb36-0b9d-4eec-8774-1f7e6d193c0f",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27714d-f406-448d-a3fa-25025b006a03",
   "metadata": {},
   "source": [
    "Categorical data is data that contains strings such as \"male\" and \"female\" or \"red\", \"green\" and \"blue\". Categorical values have to be converted into numbers. Two popular techniques for converting categorical values into numbreical values are:\n",
    "\n",
    "1. Label Encoding - replaces categorical values with integers, if there are three unique values in a column, label encoding replaces them with 0s, 1s, and 2s.\n",
    "2. One-hot encoding - adds one column to the dataset for each unique value in a categorical column and fills the encoded columns with 1s and 0s.\n",
    "\n",
    "One-hot encoding us generarly used more often as it gives every unique value an equal weight whereas label encoding implies that some values might be more important than others for example that \"male\" (1) is more important than \"female\" (0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
