{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb769fe-dc67-4173-bcaf-95ee24f99aae",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6c06b-5d08-4c7d-9cd9-3bfde48a3b62",
   "metadata": {},
   "source": [
    "Neural networks come in many varieties:\n",
    "1. Convolutional neural networks (CNNs) - used for computer-vision\n",
    "2. Recurrent neural networks (RNNs) - handwriting recognition and natural language processing\n",
    "3. Generative adversarial networks (GANs) - enable computers to create art, music or other content\n",
    "\n",
    "The simplest neural ntework is the multilayer perceptron. It consists of nodes or neurons arranged in layers. The depth of the network is the number of layers, the width is the number of neurons in each layer. The multilayer perceptron contains three layers: an input layer with two neurons, a middle layer also known as a hidden layer with three neurons, and an output layer with one neuron. The network's job is to take two floating-point values as input and produce a single floating-point number as output.\n",
    "\n",
    "Each neuron in each layer is connected to each neuron in the next layer giving rise to the term fully connected layers. Each connection is assigned a weight, which is a small floating-point number. Each neuron outside the input layer is assigned a bias, which is also a small floating-point number. In a neural network therea are also activation functions which apply simple nonlinear transforms to values propagated through the network. The most commonly used activation function is the rectified linear units (ReLU) function, which passes positive numbers through unchanged while converting negative numbers to 0s. Neurons perform simple linear transformations on data input to thme. For a neuron with a single input x, the nuron's value y is computed by multiplying x by the weight m and adding the neuron's bias b. Calculating the output with a given set of weights and biases is simple, the challenge is finding the correct values for the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1d684-c470-4e39-a34a-274a87f32905",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab0b0f-3a4e-4997-b4dc-a3eb97449c94",
   "metadata": {},
   "source": [
    "The training starts with randomly initializing weights with small random numbers and biases with 0s, at this state a neural network generates random outputs. The training samples are fed through the network, the error (difference between the computed output and the correct output) is computed using a loss function, and a backpropagation algorithm goes backward through the network adjusting the weights and biases, this is done repeatedly until the error is sufficiently small. The most critical component of backpropagation is the optimizer, which on each backward pass decides how much and in which direction, positive or negative, to adjust the weights and biases. Optimizers involve calculation of partial derivatives (calculating the slope of the contour with respect to each weight and bias), gradient descent (adjusting the weights and biases to go down the slope rather than up or sideways), and learning rates which drive the fractional adjustments made to the weights and biases in each backpropagation pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
