{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0e3517-62b9-4805-91a7-bf9c88681b20",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8e999-af98-4868-8f7a-281f5f358e78",
   "metadata": {},
   "source": [
    "Using a neural net to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e36140-c948-44e5-bfc8-5b225478acff",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88319e99-6ded-4d52-b68e-3bfd804f2d95",
   "metadata": {},
   "source": [
    "A perceptron is a type of neuron which in no longer in use by modern neural netwoks. Modern neural networks use sigmoid neurons. A perceptron takes several binary inputs and produces a single binary output. To compute the output of a perceptron the perceptron introduces weights (w), real numbers expressing the importance of the respective input to the output. The output of the perceptron depends on whether the weighted sum of the input and weights is greater than a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f3987-a2d4-4ddf-97a8-342929cf8b6e",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112079c6-406f-4e1a-a536-9ea8815b582a",
   "metadata": {},
   "source": [
    "The goal is to change the values of the weights and biases by a small amount so that the output of the network also changes by a small amount. This isn't possible with perceptrons since a small change in the value of a bias or weight of a perceptron can cause the perceptron to flip from one value to the other for example from 0 to 1 which would cause the rest of the network to change the behaviour compleately. This can be overcome by the introduction of the Sigmoid neuron which is similar to the perceptron but the difference is that it accepts inputs other than 0s and 1s and the output is not just the weigted sum of the inputs and bias but it is inserted into the sigmoid function which is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "This just means that if z is a large positive number than the output of the sigmoid neuron is close to 1, and if it is very negative then it is close to 0, but when it is a modes size number it will give a number in between 0 and 1 which is different from the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb658cd-3432-42ab-aad7-a2446643d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ae5fcb-2619-401a-ac24-bf2ff651c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eba2cc0-129c-482b-bd8f-dc34180a9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    split_index = 50000\n",
    "    x_train, x_val = x_train[:split_index], x_train[split_index:]\n",
    "    y_train, y_val = y_train[:split_index], y_train[split_index:]\n",
    "\n",
    "    training_data = (x_train.reshape(50000, 784).astype('float32') / 255, y_train.astype('int32'))\n",
    "    validation_data = (x_val.reshape(10000, 784).astype('float32') / 255, y_val.astype('int32'))\n",
    "    test_data = (x_test.reshape(10000, 784).astype('float32') / 255, y_test.astype('int32'))\n",
    "    \n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    \n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9982c84-ecb4-4502-aa0a-c7e535ac7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a720dbd7-527b-4687-8971-392c1d8fa0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f36fb9b-14e3-48e4-9120-9fc383e875a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9025 / 10000\n",
      "Epoch 1: 9244 / 10000\n",
      "Epoch 2: 9279 / 10000\n",
      "Epoch 3: 9333 / 10000\n",
      "Epoch 4: 9361 / 10000\n",
      "Epoch 5: 9345 / 10000\n",
      "Epoch 6: 9410 / 10000\n",
      "Epoch 7: 9403 / 10000\n",
      "Epoch 8: 9445 / 10000\n",
      "Epoch 9: 9405 / 10000\n",
      "Epoch 10: 9440 / 10000\n",
      "Epoch 11: 9432 / 10000\n",
      "Epoch 12: 9472 / 10000\n",
      "Epoch 13: 9475 / 10000\n",
      "Epoch 14: 9487 / 10000\n",
      "Epoch 15: 9464 / 10000\n",
      "Epoch 16: 9470 / 10000\n",
      "Epoch 17: 9494 / 10000\n",
      "Epoch 18: 9492 / 10000\n",
      "Epoch 19: 9478 / 10000\n",
      "Epoch 20: 9474 / 10000\n",
      "Epoch 21: 9502 / 10000\n",
      "Epoch 22: 9492 / 10000\n",
      "Epoch 23: 9510 / 10000\n",
      "Epoch 24: 9490 / 10000\n",
      "Epoch 25: 9482 / 10000\n",
      "Epoch 26: 9509 / 10000\n",
      "Epoch 27: 9503 / 10000\n",
      "Epoch 28: 9503 / 10000\n",
      "Epoch 29: 9513 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0,test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
