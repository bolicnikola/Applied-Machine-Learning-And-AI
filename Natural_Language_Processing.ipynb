{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44360298-2172-40aa-8f02-e9d20f5b4708",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24feb29-2597-40f6-b4db-dc179fb0fffa",
   "metadata": {},
   "source": [
    "One element that all neural networks that process text have in common is an embedding layer which uses word embeddings to transform arrays or sequences of scalar values representing words into arrays of floating-point numbers called word vectors. These vectors encode information aboout the meaning of words and relationship between them. The output of an embedding layer can be the input to a classification layer or it can be input to other types of neural network layers to tease more meaning from it before subjecting it to further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57459d0-0548-4d63-98f2-8081a88339e5",
   "metadata": {},
   "source": [
    "## Text Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fd969-eb2c-4c4c-8473-aeabd2c3d99f",
   "metadata": {},
   "source": [
    "CountVectorizer class from Scikit-Learn transforms rows of text into rows of word counts.\n",
    "1. removes punctuation and numbers\n",
    "2. converts all characters to lower-case\n",
    "3. (optional) removes stop words\n",
    "\n",
    "Vectorization is performed differently. Instead of creating a table of word counts it creates a table of sequences containing tokens representing individual words. Keras provides the Tokenizer class which is an equivalent of CountVectorizer but for deep-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee73f04c-89ee-4d08-acf0-dee7480de568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "lines = [\n",
    "    'The quick brown fox',\n",
    "    'Jumps over $$$ the lazy brown dog',\n",
    "    'Who jumps high into the blue sky after counting 123',\n",
    "    'And quickly returns to earth'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70ac0ac-6d30-40f5-924c-c2ea69cde03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 2, 5],\n",
       " [3, 6, 1, 7, 2, 8],\n",
       " [9, 3, 10, 11, 1, 12, 13, 14, 15, 16],\n",
       " [17, 18, 19, 20, 21]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3614322a-7160-48df-abb1-f741f6dc9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9b5713-09c4-4d74-81ff-2aeb7610cb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick brown fox',\n",
       " 'jumps over the lazy brown dog',\n",
       " 'who jumps high into the blue sky after counting 123',\n",
       " 'and quickly returns to earth']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944721a5-0a45-4bca-9737-943ecdcd5e60",
   "metadata": {},
   "source": [
    "Words are converted to lower case and symbols are removed but stop words and numbers are still there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7157055f-2352-4ebb-8032-0564a1b49199",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      5\u001b[39m liness = [\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mThe quick brown fox\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mJumps over $$$ the lazy brown dog\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mWho jumps high into the blue sky after counting 123\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAnd quickly returns to earth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "liness = [\n",
    "    'The quick brown fox',\n",
    "    'Jumps over $$$ the lazy brown dog',\n",
    "    'Who jumps high into the blue sky after counting 123',\n",
    "    'And quickly returns to earth'\n",
    "]\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "lines = list(map(remove_stop_words, lines))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a374a-26d5-4dea-bf2f-d083e76045cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
